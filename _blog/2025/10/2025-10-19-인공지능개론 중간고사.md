---
layout: blog
title: 인공지능개론 중간고사
date: 2025-10-19 23:00:00 +0900
categories: [LECTURE, 인공지능개론]
permalink: /blog/posts/25-10-19-TIL
use_math: true
---

## 중간고사

## 1. AI 핵심 개념 및 역사 (Lesson 1)

### 1-1. 4차 산업혁명과 AI

- 4차 산업혁명 주요 특징 :
  - **초연결 (Hyper-connectivity)** : IoT, 5G/6G로 모든 사물 연결.
  - **초지능 (Super-intelligence)** : 빅데이터(원료)와 인공지능(제작)을 통한 자동화.
  - **초융합 (Convergence)** : IT, BT, NT 등 기술 간 융합.
- 초지능 용어 비교 :
  - **슈퍼 인텔리전스 (Super Intelligence)** : AI가 인간의 모든 인지 능력을 능가하는 **특이점(Singularity)**을 통과한 상태.
  - **하이퍼 인텔리전스 (Hyper Intelligence)** : 인간 지능과 AI가 협력하여 더 스마트한 서비스를 제공하는 것 (또는 사물의 지능화).
- AI의 주요 역할 : 데이터 기반 의사결정, 반복 업무 자동화(챗봇 등), 생성형 AI(이미지/글 생성), 개인 맞춤화(추천, 정밀 의료).

### 1-2. AI 분류 체계

- 접근 방식 (4가지)
  - **Humanly Acting (사람처럼 행동)** : 인간과 구분되지 않는 행동. (예: 튜링 테스트, 챗봇) .
  - **Humanly Thinking (사람처럼 생각)** : 인간의 사고 과정 모사. (예: 인지과학, 뉴럴 네트워크) .
  - **Rationally Thinking (합리적으로 생각)** : '사고의 법칙(Laws of Thought)' 준수, 논리적 결론 도출. (예: 전문가 시스템, 추론 엔진) .
  - **Rationally Acting (합리적으로 행동)** : 목표 달성을 위한 최적의 행동 수행. (예: 합리적 에이전트, 자율주행차) .
- 지능 범위 (강도)
  - **약한 AI (Weak AI)** : 특정 작업에 한정됨. 자의식 없음. (예: 음성 인식, 추천 시스템. **이미 실현됨**) .
  - **강한 AI (Strong AI)** : 인간 수준의 범용 지능. 이해, 추론, 자기 인식 가능. (연구/이론 단계) .
  - **초강한 AI (Super AI)** : 인간을 초월한 지능. (가설적 미래 개념) .

### 1-3. AI, ML, DL의 포함 관계

- **AI (Artificial Intelligence)** : 가장 넓은 범위. 지능을 다루는 모든 연구.
- **ML (Machine Learning)** : AI의 하위 분야. 기계가 데이터로부터 **스스로 학습**하게 하는 연구.
- **DL (Deep Learning)** : ML의 하위 분야. ML을 **'인공신경망(Neural Networks)'**을 깊게 쌓아 구현하는 기술.

### 1-4. 머신러닝 3대 유형

- **지도 학습 (Supervised Learning)** : **'정답'(레이블)이 표시된** 데이터를 사용. (예: '개/고양이' 사진에 레이블을 붙여 학습). **분류(Classification)**와 **회귀(Regression)**가 대표적.
- **비지도 학습 (Unsupervised Learning)** : **정답 레이블이 없는** 데이터를 사용. 데이터 내 숨겨진 구조나 패턴을 찾음. (예: **군집화(Clustering)**, 차원 축소).
- **강화 학습 (Reinforcement Learning)** : 에이전트(Agent)가 환경 내에서 행동하고, 그 결과로 얻는 **보상(Reward)을 최대화**하는 방향으로 학습. (예: 게임 AI, 로봇 제어) .

### 1-5. 주요 역사적 이벤트 (시간순)

- **1950년** : 앨런 튜링, **튜링 테스트** 제안.
- **1956년** : 존 마카시, '**AI**' 용어 첫 사용.
- **1950s~** : 로젠블랏, **퍼셉트론(Perceptron)** 개발 (신경망). / **전문가 시스템** (MYCIN) 등장 (규칙기반).
- **1997년** : IBM **딥 블루** (체스 챔피언 격파).
- **2011년** : IBM **왓슨** (퀴즈쇼 '제퍼디' 우승).
- **2012년** : **알렉스넷(AlexNet)** (이미지넷 대회 우승) → **딥러닝 시대 개막**.
- **2016년** : 구글 딥마인드 **알파고** (이세돌 9단 승리).

---

## 2. AI 응용 및 융합 (Lesson 2-1)

### 2-1. 핵심 용어

- **리걸테크 (LegalTech)** : 법률(Legal)과 기술(Technology)의 합성어.
- **휴머노이드 (Humanoid)** : 인간 모습으로 교감 가능한 로봇 (예: 소피아(Sophia)).
- **바이오인포매틱스 (Bioinformatics)** : 생물학(Biology)과 소프트웨어(Software)가 융합된 생명정보과학.
- **오픈소스 (Open Source)** : 소스 코드 무료 공개.
  - **기업 공개 이유** : AI **생태계를 만들어 선도**하기 위해.
- **주요 AI 도우미 (참고)** : 구글 어시스턴트, MS 코타나, 애플 시리, 아마존 알렉사, 삼성 빅스비.
- **주요 오픈소스 AI SW (참고)** : 구글 텐서플로, 페이스북 빅서, 바이두 WARP-CTC.

### 2-2. AI 응용 및 융합 사례

- **광고/추천** : 유튜브, 넷플릭스 (원리: 사용자 행동 데이터 분석).
- **금융** : 신용평가, 금융 사기 탐지(Fraud Detection) (원리: 거래 패턴 학습 후 이상 탐지).
- **의료** : AI 영상진단 (X-ray, MRI 판독), IBM Watson for Oncology (암 환자 치료).
- **예술 (생성형 AI)** : DALL·E, MidJourney (이미지 생성), ChatGPT (시나리오 작성).
- **법률 (리걸테크)** : AI 변호사가 판례 정보 제공 및 법률 자문 보조.
- **로봇공학** : 휴머노이드 '소피아(Sophia)' (60여 개 감정 표현 및 대화).
- **농업** : 스마트팜 (AI+IoT 적용), 농업용 로봇 (과일 수확, 사료 공급).
- **인문학** : 스탠포드 인간중심 인공지능연구소(HAI) 설립.
- **기타 분야** : 음성 비서, 자율주행, 비즈니스(유가 예측, 소비자 분석).

---

## 3. AI 방법론 심화 (Lesson 2-2)

### 3-1. AI 3대 분류 체계

- **기호주의 (Symbolism)** : 규칙기반 AI.
- **연결주의 (Connectionism)** : 신경망 기반 AI (Neural Network).
- **통계적 AI (Statistics)** : 머신러닝 (HMM, SVM 등).

### 3-2. 기호주의 vs. 연결주의

- **규칙기반 인공지능 (기호주의)**
  - **정의** : 기호(Symbol)와 논리(Logic) 위주의 규칙기반 AI. '초기 인공지능'.
  - **특징** : 연역 추리, 논리적 추론.
  - **장점** : 문제 해결 결과에 대한 **입증이나 설명**이 용이.
  - **한계** : 학습(Learning) 구현이 어렵고, 문자/음성/영상 등 **패턴인식 면에서 한계**.
  - **주요 연구 분야** : **전문가 시스템** (예: 의사 진단 '마이신'), 수학 정리 증명, 게임.
- **신경망 기반 인공지능 (연결주의)**
  - **정의** : 인공신경망(ANN). 인간 두뇌 뉴런 연결에서 아이디어를 따옴.
  - **주요 역사** :
    - **1957년**: 로젠블럿, **퍼셉트론(Perceptron)** 개발.
    - **1969년**: 『퍼셉트론즈』 출판 후 1차 AI 겨울 (쇠퇴).
    - **1986년**: 다층 퍼셉트론과 **역전파(Backpropagation) 알고리즘** 개발로 제2의 도약.
    - **2004년**: 힌튼 교수, **딥러닝(Deep Learning)** 학습 알고리즘 제안.
  - **주요 연구 분야** : **패턴 인식** (문자인식, 음성인식, 영상인식).

### 3-3. ML vs. DL (자질 설계 관점)

- **머신러닝 (ML)** : [입력] → [특징 추출] → [분류] → [출력]
  - **특징 추출(Feature extraction)**과 **분류(Classification)** 단계가 **분리됨**.
  - **자질 설계(Feature Design)**의 능숙도에 따라 성능이 좌우됨.
- **딥러닝 (DL)** : [입력] → [특징 추출 + 분류] → [출력]
  - **특징 추출과 분류**가 신경망 내에서 **통합되어** 수행됨.
  - 컴퓨터가 **스스로 자질을 학습**하여 설계함.

### 3-4. 기타 인물 및 용어

- **앨런 튜링 (Alan Turing)** : '컴퓨터과학의 아버지'. '튜링 머신' 창안.
- **튜링 테스트 (Turing Test)** : 1950년 튜링이 제안한 AI 성능 테스트. 상대방이 기계인지 사람인지 **눈치채지 못할 확률**로 성능 측정.
- **엘라이자 (ELIZA)** : 1966년 개발된 초기 챗봇. 실제 지능이 아닌 **'패턴의 매칭'**으로 AI처럼 보이게 함.
- **유진 구스트만 (Eugene Goostman)** : 2014년 '우크라이나 13세 소년'으로 자신을 소개해 튜링 테스트를 통과했다고 알려진 프로그램.
- **기타 주요 분야** :
  - **자연어 처리 (NLP)** : 인간 언어(자연어)를 컴퓨터가 이해/해석/생성.
  - **컴퓨터 비전 (CV)** : 컴퓨터가 이미지/비디오를 보고 이해.
  - **멀티모달 AI (Multimodal AI)** : 여러 종류 데이터(텍스트, 이미지 등)를 동시 이해 (예: Text-to-Image).

---

## 4. 지식과 추론 (Lesson 3)

### 4-1. 지식의 정의와 종류

- **데이터 피라미드 (DIKW)** :
  - **데이터 (Data)** : 가공되지 않은 관측값.
  - **정보 (Information)** : 데이터에 목적이나 의미를 부여한 것.
  - **지식 (Knowledge)** : 정보를 취합, 분석하여 사람이 이해한 것.
  - **지혜 (Wisdom)** : 지식보다 높은 수준의 통찰.
- **지식의 종류** :
  - **형태 기준** :
    - **암묵지 (Tacit)** : 표현하기 어려운, 경험적/학습적 지식.
    - **형식지 (Explicit)** : 쉽게 형식을 갖추어 표현 가능한 지식.
  - **내용 기준** :
    - **절차적 지식 (Procedural)** : 문제 해결의 '절차' (How).
    - **선언적 지식 (Declarative)** : 대상의 '성질, 관계' (What).

### 4-2. 전문가 시스템 (Expert System)

- **정의** : 특정 분야에서 인간 전문가의 지식을 활용하는 AI 소프트웨어의 최초 성공 형태.
- **핵심 구성 요소** :
  - **지식 베이스 (Knowledge Base)** : 전문가의 지식을 IF-THEN 규칙 형태로 저장.
  - **추론 엔진 (Inference Engine)** : 지식 베이스의 규칙과 주어진 사실(Fact)을 비교하여 결론 도출.
  - **(참고) 사실 (Fact)** : 현재 알려진 데이터나 정보 (단기 기억).

### 4-3. 지식 표현 방법

- **규칙 (Rule)** :
  - IF-THEN 형태의 조건부 문장. (IF: 조건부, THEN: 결론부).
  - **5가지 표현 방식** :
    1.  **인과관계** : (IF 원인 THEN 결과).
    2.  **추천** : (IF 상황 THEN 추천 내용).
    3.  **지시** : (IF 상황 THEN 지시 내용).
    4.  **전략** : 일련의 규칙들로 절차 표현.
    5.  **휴리스틱 (Heuristic)** : '경험적 지식' 표현 (최적을 보장하진 않음).
- **프레임 (Frame)** :
  - 특정 객체/개념을 **슬롯(Slot) - 값(Value)** 구조로 표현.
  - **상속(Inheritance)**을 지원.
  - **클래스 프레임 (Class)** : '부류'에 대한 정보 (설계도).
  - **인스턴스 프레임 (Instance)** : '특정 객체'에 대한 정보 (실체).
- **논리 (Logic)** :
  - **명제 논리** : 참/거짓 판정이 가능한 '문장' 자체를 P, Q 등 기호와 논리 기호(¬, ∨, ∧, →)로 다룸.
  - **술어 논리** : 명제 논리를 확장하여 문장의 '내용'을 다루기 위해 **변수, 함수, 관계**를 도입. (예: 아버지(김춘추, 영철)).

### 4-4. 추론 방식

- **연역 추론 (Deduction)** : '일반 규칙'에서 '구체적인 결론' 도출 (예: 삼단논법). 논리의 일관성이 장점.
- **귀납 추론 (Induction)** : '개별적인 사실들'로부터 '일반적인 결론' 도출. 결론을 100% 보장할 수 없음.
- **전향 추론 (Forward Chaining)** :
  - **정의** : 알려진 사실(Facts)로부터 출발하여 결론을 이끌어내는 방법.
  - **방식** : "데이터 주도형 (Data-driven)" (증거 → 결론).
  - **흐름** : (순방향) A가 사실 → A→X 규칙 발동 → X가 사실이 됨 ....
  - **적합 분야** : 진단, 이벤트 감지, 예측.
- **후향 추론 (Backward Chaining)** :
  - **정의** : 목표(Goal)를 설정하고 이를 증명하는 증거(Facts)를 찾는 방법.
  - **방식** : "목표 주도형 (Goal-driven)" (목표 → 증거 찾기).
  - **흐름** : (역방향) 목표 Z? → Y&C&D→Z 규칙 확인 → Y, C, D 필요 ....
  - **적합 분야** : 목표 달성, 계획 수립, "Yes/No" 증명.

---

## 5. 탐색 알고리즘 (Lesson 4-1)

### 5-1. 탐색의 기본 정의

- **탐색 (Search)** : **상태 공간(State Space)**에서 **시작 상태(Initial State)**로부터 **목표 상태(Goal State)**까지의 경로를 찾는 것.
- **문제 정의 5요소** :
  1.  **초기 상태 (Initial State)** : 문제의 시작점.
  2.  **상태 공간 (State Space)** : 가능한 모든 상태의 집합.
  3.  **연산자 (Operators)** : 상태를 바꾸는 규칙 (예: 8-퍼즐에서 빈칸 이동).
  4.  **목표 상태 (Goal State)** : 문제의 해결 상태.
  5.  **비용 함수 (Path Cost)** : 상태 전이에 드는 비용.
- **탐색 알고리즘 분류** :
  - **맹목적 탐색 (Uninformed Search)** : 목표 상태 외 정보 없이, 정해진 순서(레벨, 깊이)로 탐색.
  - **경험적 탐색 (Informed Search)** : 휴리스틱 정보를 사용해 탐색 범위를 좁힘.
- **휴리스틱 (Heuristic)** :
  - 경험에 기반한 직관적 판단.
  - AI 탐색에서는 목표까지의 **예상 비용($h(n)$)** 함수로 사용.
  - (예: "목적지까지의 직선 거리", "제자리에 없는 타일 개수").
- **A\* 평가 함수** : $f(n) = g(n) + h(n)$.
  - $g(n)$: (실제 비용) 시작 노드에서 $n$까지 **이미 투입된 비용**.
  - $h(n)$: (추정 비용) $n$에서 목표까지의 **남은 예상 비용** (휴리스틱).
- **허용성 (Admissibility)** : $h(n)$ (추정치)이 $h^\*(n)$ (실제 비용)보다 항상 작거나 같은 조건 ($h(n) \le h^\*(n)$). 이 경우 A\*는 최적해를 보장.

### 5-2. 맹목적 탐색 (Uninformed Search)

- **너비 우선 탐색 (BFS)** :
  - **개념** : '낮은 깊이(level)'부터 순서대로 탐색.
  - **자료구조** : **큐 (Queue)** (FIFO).
  - **최적해** : **보장됨** (가장 얕은 해를 먼저 발견).
  - **메모리** : **비효율적** (같은 레벨의 노드들을 모두 보관해야 함).
- **깊이 우선 탐색 (DFS)** :
  - **개념** : '한 경로의 끝까지' 먼저 탐색 (막히면 백트래킹).
  - **자료구조** : **스택 (Stack)** (LIFO) 또는 **재귀(Recursion)**.
  - **최적해** : **보장되지 않음** (깊은 경로의 해를 먼저 찾을 수 있음).
  - **메모리** : **효율적** (현재 경로의 노드만 저장).
- **반복적 깊이심화 탐색 (IDS)** :
  - **개념** : DFS에 **깊이 한계($L$)**를 0, 1, 2...로 늘려가며 반복 수행.
  - **자료구조** : 내부적으로 DFS (스택, 재귀) 사용.
  - **최적해** : **보장됨** (BFS처럼 얕은 곳부터 찾음).
  - **메모리** : **효율적** (DFS의 장점).
  - **결론** : BFS의 최적해 보장과 DFS의 메모리 효율성을 모두 가짐.

### 5-3. 경험적 탐색 (Informed Search)

- **A\* (A-Star) 알고리즘** :
  - 가장 효과적인 경로 탐색 중 하나.
  - 평가 함수 $f(n) = g(n) + h(n)$의 값이 **가장 작은 노드**를 우선 탐색.
  - 휴리스틱 $h(n)$이 **허용성**을 만족하면 **최적해를 보장**.
- **언덕 오르기 (Hill Climbing)** :
  - 이웃 노드 중 휴리스틱 값이 **가장 좋은(높은) 노드 하나만** 선택해 이동.
  - 메모리 사용이 적고 빠름.
  - **치명적 단점**: **'지역 최적점(Local Optima)'**에 갇힐 수 있음 (더 좋은 해가 있어도 멈춤).
- **최상 우선 탐색 (Best-First Search)** :
  - 가장 유망해 보이는(평가 함수 $f(n)$ 값이 작은) 노드를 먼저 확장하는 방식의 통칭.
  - **Greedy Best-First** : $f(n) = h(n)$ (오직 예상 비용만 고려, 빠르지만 최적해 보장 안 됨).
  - **A\* 탐색** : $f(n) = g(n) + h(n)$ (최상 우선 탐색의 일종).

---

## 6. 적대적 탐색과 CSP (Lesson 4-2)

### 6-1. 핵심 용어

- **적대적 탐색 (Adversarial Search)** : 두 명 이상의 플레이어가 서로 이익이 충돌하는 환경(예: 체스)에서 최적의 수를 찾는 과정.
- **게임 트리 (Game Tree)** : 게임의 상태(노드)와 진행을 트리로 표현한 것.
  - **노드 (Node)** : 게임의 특정 상태.
  - **리프 (Leaf)** : 게임이 종료된 상태 (승/패/무).
- **Minimax 전략** :
  - **MAX Player** : 자신의 이득(점수)을 **최대화(Maximize)**하려는 플레이어 (나).
  - **MIN Player** : 상대(MAX)의 이득을 **최소화(Minimize)**하려는 플레이어 (상대방).
- **$\alpha-\beta$ 가지치기 (Alpha-Beta Pruning)** :
  - Minimax 탐색 시 불필요한 탐색을 중단(가지치기)하여 효율을 높이는 기법.
  - **$\alpha$ (Alpha)** : MAX가 현재까지 찾은 최선의 값 (최소 보장 이득, **하한선**).
  - **$\beta$ (Beta)** : MIN이 현재까지 찾은 최선의 값 (최대 허용 손실, **상한선**).
- **몬테카를로 트리 탐색 (MCTS)** :
  - 바둑처럼 복잡한 게임에서 **무작위 시뮬레이션(랜덤 플레이아웃)**을 반복하여 승리 확률을 계산. (AlphaGo 핵심) .
  - **탐색(Exploration)과 활용(Exploitation)의 균형**이 중요.
  - **활용 (Exploitation)** : 현재까지 승률이 가장 좋았던 경로 선택.
  - **탐색 (Exploration)** : 아직 시도하지 않은 새로운 경로 탐험.
  - **UCT 공식** : $UCB1 = \frac{w_i}{n_i} + c\sqrt{\frac{\ln N}{n_i}}$.
    - $\frac{w_i}{n_i}$: **활용** 부분 (현재 승률).
    - $c\sqrt{\frac{\ln N}{n_i}}$: **탐색** 부분 (방문 횟수 $n_i$가 적을수록 커짐).
- **제약조건 만족 문제 (CSP)** :
  - 여러 변수와 **제약 조건(규칙)**을 모두 만족하는 해를 찾는 문제.
  - (예: 8-Queens 문제, 문자 암호 풀이).

### 6-2. Minimax 알고리즘 작동 원리

1.  **리프 노드 평가** : 게임 종료 상태(리프)에 점수 부여 (승 +1, 패 -1 등).
2.  **값의 역전파** :
    - **MIN 노드 (상대 턴)** : 자식 노드들 중 **가장 작은(min) 값**을 선택.
    - **MAX 노드 (내 턴)** : 자식 노드들 중 **가장 큰(max) 값**을 선택.
3.  **최종 결정** : 루트 노드까지 반복 후, 루트(MAX)는 자식 노드들 중 **가장 큰 값**을 가진 수를 선택.

### 6-3. $\alpha-\beta$ 가지치기

- Minimax와 **동일한 결과를 보장**하지만, 불필요한 노드 탐색을 중단하여 속도를 향상.
- **핵심 가지치기 조건** : $\alpha \ge \beta$.
  - $\alpha$(MAX의 하한선)가 $\beta$(MIN의 상한선)보다 크거나 같아지는 순간, 해당 노드의 나머지 자식(형제) 노드들은 탐색을 중단.

### 6-4. 몬테카를로 트리 탐색 (MCTS) 4단계

1.  **선택 (Selection)** : 루트에서 시작하여 UCT 값이 가장 높은 자식 노드를 따라 리프까지 이동.
2.  **확장 (Expansion)** : 선택된 리프에서 시도하지 않은 새 수(자식 노드)를 하나 생성.
3.  **시뮬레이션 (Simulation)** : 새 노드에서 게임 끝까지 **무작위로** 수를 두어 가상 게임 진행.
4.  **역전파 (Backpropagation)** : 시뮬레이션 결과(승/패)를 지나온 모든 부모 노드에 거꾸로 전파 (승률 $w_i$, 방문 수 $n_i$ 업데이트).

### 6-5. 고전적 CSP 예시

- **상자 쌓기 문제** : '작은 상자는 큰 상자 밑에 못 간다'는 규칙 하에 목표 달성.
- **8-Puzzle 문제** : '빈칸은 상하좌우 이동' 제약 하에 목표 배열 찾기.
- **물통 문제** : '가득 채우기', '비우기' 등 정해진 규칙만으로 특정 용량 만들기.
- **문자 암호 풀이** : '각 문자는 0~9 사이 서로 다른 숫자' 제약 하에 수식 풀기.
- **늑대, 염소, 양배추 / 선교사와 식인종** : '늑대와 염소만 두면 안 됨' 또는 '식인종 수가 선교사 수보다 많으면 안 됨' 제약 하에 강 건너기.
- **N-Queens 문제** : '퀸끼리 서로 공격할 수 없도록' 배치하는 제약 문제.

---

## 7. 머신러닝의 기본 개념 (Lesson 5-1)

### 7-1. 전통적 프로그래밍 vs. 머신러닝

- **전통적 프로그래밍** :
  - 입력: 데이터 + **규칙 (Rule)**
  - 출력: 해답 (Output)
  - 특징: 사람이 모든 규칙을 직접 작성합니다.
- **머신러닝** :
  - 입력: 데이터 + **해답 (Output)**
  - 출력: **규칙 (Rule)**
  - 특징: 입출력 데이터 간의 관계를 스스로 학습하여 규칙을 생성합니다.

### 7-2. 머신러닝의 3대 학습 유형

- **지도 학습 (Supervised Learning)**
  - **정의** : '레이블(정답)'이 표시된 데이터를 사용하여 모델을 학습시킵니다.
  - **목표** : $y=f(x)$ 함수를 학습하여 미래 데이터를 예측합니다.
  - **주요 유형** :
    - **분류 (Classification)** : 데이터를 미리 정해진 범주(Category)로 나눕니다. (예: 스팸 메일 분류, 질병 진단)
    - **회귀 (Regression)** : 연속적인 수치(Value)를 예측합니다. (예: 주가 예측, 날씨 예측)
- **비지도 학습 (Unsupervised Learning)**
  - **정의** : '레이블(정답)이 없는' 데이터를 사용하여 데이터 내 숨겨진 구조나 패턴을 발견합니다.
  - **목표** : 데이터의 구조나 분포($x \sim p(x)$)를 파악합니다.
  - **주요 유형** :
    - **군집화 (Clustering)** : 비슷한 특성의 데이터끼리 그룹으로 묶습니다. (예: 고객 세분화)
    - **차원 축소 (Dimensionality Reduction)** : 데이터의 특징을 요약하거나 줄입니다. (예: PCA, 특징 추출)
- **강화 학습 (Reinforcement Learning)**
  - **정의** : **에이전트(Agent)**가 **환경(Environment)**과 상호작용하며, 그 결과로 얻는 **보상(Reward)**을 최대화하는 방향으로 학습합니다.
  - **목표** : 의사결정을 위한 최적의 **행동 정책(Policy)**을 찾습니다.
  - **응용** : 게임 AI(AlphaGo), 로봇 제어, 자율주행.

### 7-3. 기타 학습 유형

- **준지도 학습 (Semi-Supervised Learning)** : '일부만 라벨링된' 데이터와 다수의 비라벨 데이터를 함께 활용하여 라벨링 비용을 절감합니다.
- **자기지도 학습 (Self-Supervised Learning)** : 데이터 자체에서 '인공적인(가상의) 라벨'을 만들어 학습합니다. (최근 LLM의 핵심 기술)

---

## 8. 지도 학습: 회귀 (Lesson 5-2)

### 8-1. 핵심 용어 (복습 및 심화)

- **지도 학습** : 입력(X)과 정답(y)의 관계($f(x) \approx y$)를 학습합니다.
- **분류 (Classification)** : 이산적인 **범주(Category)**로 예측합니다. (예: 스팸/정상)
- **회귀 (Regression)** : 연속적인 **수치(Value)**를 예측합니다. (예: 집값 예측)
- **독립 변수 (Independent Variable)** : 원인이 되는 변수 (설명 변수, X). (예: 키)
- **종속 변수 (Dependent Variable)** : 결과가 되는 변수 (반응 변수, Y). (예: 몸무게)
- **단순 회귀 (Simple Regression)** : 독립 변수가 1개.
- **다중 회귀 (Multiple Regression)** : 독립 변수가 2개 이상.
- **선형 회귀 (Linear Regression)** : 변수 간 관계가 **직선(1차 함수)**이라고 가정합니다.
  - **회귀직선 (Regression Line)** : $y=a+bx$ 형태의 직선.
- **피팅 (Fitting)** : 모델 예측값과 실제 값의 차이(오차)를 최소화하도록 모델 파라미터(a, b)를 조정하는 과정입니다.
- **최소제곱법 (Least Squares Method / OLS)** : **오차(잔차)의 제곱합($\sum \epsilon_i^2$)**을 최소화하는 방식으로 회귀 계수(a, b)를 추정합니다.
- **비용 함수 (Loss Function) / MSE** : 모델의 오차를 측정하는 함수. 선형 회귀에서는 주로 **평균 제곱 오차(MSE)** $MSE = \frac{1}{n}\sum(y_i - \hat{y}_i)^2$ 를 사용하며, 이 값을 최소화하는 것이 목표입니다.
- **결정계수 ($R^2$)** : 모델이 종속 변수의 변동성을 '얼마나 잘 설명하는지' 나타내는 지표 (0~1 사이, 1에 가까울수록 좋음).
- **수정된 결정계수 (Adjusted $R^2$)** : 독립 변수의 개수가 많아지면 $R^2$가 커지는 경향을 보완하기 위해, **독립 변수의 개수를 고려하여 조정한** 결정계수입니다. 모델 간 비교 시 유용합니다.
- **다중공선성 (Multicollinearity)** : 다중 회귀에서 **독립 변수들 간에 높은 상관관계**가 나타나는 문제입니다. 이는 회귀 계수 추정을 불안정하게 만듭니다.

### 8-2. 지도 학습 장단점

- **장점** :
  - 입력-출력 관계가 명확하여 직관적입니다.
  - 데이터가 충분하면 높은 예측 정확도를 얻을 수 있습니다.
  - Accuracy, RMSE 등 명확한 지표로 성능 평가가 용이합니다.
- **단점** :
  - 정답(레이블) 데이터 확보 비용이 큽니다.
  - 학습 데이터에 없는 새 패턴에 대한 일반화 성능이 부족할 수 있습니다.
  - **과적합(Overfitting)** 위험이 있습니다 (학습 데이터에만 너무 맞춰짐).
  - 데이터가 편향되면 모델도 편향될 수 있습니다.

### 8-3. 단순 선형 회귀 모형

- **모형** : $Y = \beta_0 + \beta_1 X + \epsilon$
  - $\beta_0$: 절편, $\beta_1$: 기울기, $\epsilon$: 예측 불가능한 오차항.
- **가정** : 오차항 $\epsilon_i$는 평균이 0이고 분산이 $\sigma^2$인 정규분포를 따르며 서로 독립적이라 가정합니다.
- **목표** : 데이터를 가장 잘 설명하는 회귀 계수의 **추정값($a, b$)**을 찾는 것입니다.
- **추정 (최소제곱법)** : 잔차($e_i = y_i - \hat{y}_i$)의 제곱합 $\sum e_i^2$을 최소화하는 $a$와 $b$를 찾습니다.

### 8-4. 다중공선성 문제

- **문제점** : 독립 변수 간 상관이 너무 높으면, 각 변수의 순수한 영향을 분리하기 어려워 계수 추정이 불안정해지고 해석이 어려워집니다.
- **진단 방법** :
  - 독립 변수 간 상관계수 확인.
  - **분산팽창지수 (VIF)** 계산. (일반적으로 VIF > 10 이면 의심)
- **해결 방법** :
  - 문제를 일으키는 변수 제거.
  - **주성분 분석 (PCA)**으로 변수 변환.
  - **능형 회귀 (Ridge Regression)** 등 규제 기법 사용.

---

## 9. 지도 학습: 로지스틱 회귀 (Lesson 5-3)

### 9-1. 핵심 용어

- **로지스틱 회귀 (Logistic Regression)** :
  - 선형 회귀를 변형하여 **분류(Classification)** 문제 (특히 이진 분류)를 해결하는 알고리즘입니다.
  - 출력으로 **확률값(0~1 사이)**을 예측합니다.
- **시그모이드 함수 (Sigmoid Function)** :
  - 로지스틱 회귀의 핵심 함수.
  - 입력값(선형 회귀식의 결과 $z$)을 0과 1 사이의 값으로 변환하여 확률처럼 해석하게 합니다.
  - 식: $y = \frac{e^z}{1+e^z}$
- **오즈 (Odds)** :
  - 어떤 사건이 일어날 확률($P$)과 일어나지 않을 확률($1-P$)의 비율입니다.
  - 식: $\frac{P}{1-P}$
- **로짓 변환 (Logit Transformation)** :
  - 성공 확률 $P$를 **로그 오즈(Log Odds)** 값으로 변환하는 과정입니다.
  - 비선형적인 $P$ (0~1)를 선형적인 $X$ 변수들과 연결해줍니다.
  - 식: $logit(P) = log(\frac{P}{1-P}) = \beta_0 + \beta_1 X_1 + ...$
- **오즈비 (Odds Ratio, OR)** :
  - 독립 변수 $X_i$가 **1 단위 증가**했을 때, **성공 오즈(Odds)가 몇 배로 변하는지** 나타내는 값입니다.
  - 로지스틱 회귀 계수 $\beta_i$에 지수 함수를 적용한 값($e^{\beta_i}$)과 같습니다.
  - OR > 1: $X_i$ 증가 시 성공 확률 높아짐.
  - OR < 1: $X_i$ 증가 시 성공 확률 낮아짐.
  - OR = 1: $X_i$가 성공 확률에 영향 없음.

### 9-2. 분류에 로지스틱 회귀를 쓰는 이유

- **선형 회귀의 한계** : 선형 회귀의 출력값($-\infty, +\infty$)은 0 또는 1로 분류하기에 부적합하며, 0보다 작거나 1보다 큰 값이 나와 해석이 어렵습니다.
- **로지스틱 회귀의 해결** : 선형 회귀의 결과값을 **시그모이드 함수**에 통과시켜 항상 **0과 1 사이의 확률값**으로 만듭니다. 이 확률을 임계값(보통 0.5) 기준으로 0 또는 1로 분류합니다.

### 9-3. 로지스틱 회귀의 종류

- **이진 (Binary)** : 결과가 0/1 두 가지인 경우. (예: Spam/Not Spam)
- **다항 (Multinomial)** : 결과가 3가지 이상이고 **순서가 없는** 경우. (예: 고양이/개/토끼)
- **순서 (Ordinal)** : 결과가 3가지 이상이고 **순서가 있는** 경우. (예: Bad/Neutral/Good)

### 9-4. 오즈비(Odds Ratio) 해석

- **필요성** : 계수 $\beta_i$ 자체는 '로그 오즈'의 변화량이므로 직관적 해석이 어렵습니다.
- **해석** : $e^{\beta_i}$ (오즈비)를 계산하면, "다른 변수가 일정할 때, 해당 변수 $X_i$가 1단위 증가하면 성공(Y=1) 오즈가 몇 배가 되는가?"를 알 수 있습니다.
- **(예시)** : 타이타닉 데이터에서 '성별(남=1)' 변수의 오즈비가 0.0717이라면, 다른 조건이 같을 때 남성은 여성보다 생존할 오즈가 0.0717배 (즉, 생존 가능성이 매우 낮음)임을 의미합니다.

---

## 10. 추가 용어 정리 (Lesson 5-4)

- **Scikit-learn (사이킷런)** : Python 머신러닝 라이브러리.
- **Diabetes dataset** : Scikit-learn 내장 당뇨병 예제 데이터셋.
- **train_test_split** : 데이터를 훈련용과 테스트용으로 나누는 함수.
- **Pipeline** : 데이터 전처리(예: 스케일링)와 모델 학습 단계를 연결하는 도구.
- **StandardScaler** : 데이터를 표준화(평균 0, 표준편차 1)하는 전처리기.
- **잔차 (Residual)** : 실제 값($y$)과 모델 예측값($\hat{y}$)의 차이 ($y - \hat{y}$).
- **MAE (Mean Absolute Error)** : 오차 절대값의 평균.
- **RMSE (Root Mean Squared Error)** : 평균 제곱 오차(MSE)의 제곱근.
