---
layout: blog
title: 신경망
date: 2025-11-25 12:00:00 +0900
categories: [LECTURE, 인공지능개론]
permalink: /blog/posts/25-11-25-TIL
use_math: true
---

## 인공지능개론(정명희) 13주차 강의내용

### 1. 신경망의 개요와 역사

- 신경망은 인간 두뇌의 생물학적 뉴런의 작용을 모방한 모델<br>
- 뉴런들로부터의 입력을 일정한 함수를 거쳐 출력<br>
- ‘인공신경망’(Artificial Neural Networks)으로 부르기도 함<br>
- 신경망은 병렬성(parallelism)이 뛰어남<br>
- 문자인식, 음성인식, 영상인식, 자연어 처리 등에 이용

**(1) 근대 신경망 연구의 시작**

- 1943년 워렌 맥쿨로치 (Warren McCulloch)와 월터 피츠 (Walter Pitts)의 \<A logical calculus of ideas immanent in nervous activity\> 로부터 시작
- 각 신경세포 (neuron) 의 기능은 매우 단순하나, 이들이 상호 연결됨으로써 복잡한 계산을 수행하는 신경 시스템의 기초 마련

**(2) Rosenblatt의 perceptron**

- 신경세포와 유사한 단순 계산기능을 갖는 요소로 구성된 입력층과 출력층을 갖는 신경 시스템의 모델 제시
- 입력과 출력 사이의 synapse 를 출력층의 제곱오차가 최소가 되는 방법으로 학습시킬 수 있음을 보여 줌

**(3) 신경망의 암흑기**

- perceptron 모델이 선형분리 기능밖에 없고 많은 실제 문제를 해결하지 못한다는 것을 증명함으로 가능성에 대한 회의

**(4) 신경망의 재부흥**

- 1982년 칼텍 (Cal Tech)의 존 홉필드(John Hopfield)가 공헌
- 신경망 연구를 공학적 관점에서 접근 -> 음성인식, 문자인식, 최적화 등 응용문제의 연구 활발<br>
  ① 오차역전파 (error backpropagation) 에 의한 다층구조 perceptron(multi-layer perceptron) 의 학습방법 발견<br>
  ② 급속한 컴퓨터 발달에 의한 대규모 시뮬레이션의 빠른 처리<br>
  ③ 신경생리학의 발달 -> 이로부터 많은 것을 배움<br>
  ④ VLSI 및 광학기술의 발달로, 신경망의 장점인 병렬 특성을 살리는 구현 가능, 슈퍼컴퓨터를 능가하는 신경망 칩의 등장<br>
  ⑤ 기존의 AI 의 한계 -> 적응학습 능력과 병렬구조를 갖는 신경망의 부각<br><br>

### 2. 뇌의 동작원리

**실제의 신경망**

- 인간을 비롯한 동물들의 사고활동을 관장하는 뇌는, 수많은 신경 세포들의 연결로 구성
- 뉴런이라는 최소단위의 세포와 시냅스라 불리는 연결구조로 구성
- 인간의 뇌는 천억 개의 뉴런과 십 조개의 시냅스로 구성

**실제 뉴런의 구조**

- 수상돌기는 자극의 입력을, 축색돌기는 자극의 전달을 담당하며, 축색돌기의 끝 신경말단에서 자극의 출력이 이루어진다.

**생물학적 신경망**

- **뇌의 구조**
  - 수많은 뉴런(신경세포)들이 시냅스(synapse)를 통해 망의 구조를 이루면서 여러 가지 기능 수행
- **뉴런의 구조**

  **1. 세포체(soma)**

  - 신경세포의 중앙에 위치
  - 정보의 수용, 연산처리, 출력의 전송

  **2. 수상돌기(dendrite)**

  - 나뭇가지처럼 넓게 퍼져 있는 두껍고 짧은 다수의 돌기
  - 다른 뉴런과 연결되어 입력신호 받

  **3. 축색(돌기)(axon) : 한 개의 가느다란 섬유**

  - 세포체에 붙어 있으며 전기적으로 활성화 되고, 뉴런에 의해 발생되는 펄스를 다른 뉴런들에 전달
  - 축색의 끝부분은 가느다란 가지로 나누어져 있고, 다른 뉴런의 수상돌기와 시냅스(synapse)를 통해 연결
  - 뉴런간의 정보교환 : 시냅스를 통해 이루어짐
  - 사람의 대뇌피질에는 약 1000억(1011)개의 뉴런, 각 뉴런은 약 1천개의 수상돌기 가짐, 100조(1014)개의 시냅스
  - 뉴런의 주요기능 : 활동 전위의 전기펄스 생성과 전송

**뉴런의 동작**

- 뉴런이 흥분하게 되면, 그 자극은 축색돌기말단에서 분비되는 화학물질에 의해 다음 뉴런의 수상돌기로 전달(어떤 화학물질은 다른 뉴런의 흥분에 기여하고, 어떤 것은 억제하는데 기여)
- 뉴런은 입력자극의 크기에 따라 다른 출력을 나타냄
- 일정한 역치와 입력값을 비교하여 출력을 ON/OFF - 역치함수에 의한 것이 대표적

**인공신경망**

- 인간 뇌의 특징
  - 인간의 뇌는 매우 복잡하고, 비선형적이며, 병렬적인 정보 처리 시스템
  - 정보는 신경망 전체에 동시에 저장되고 처리됨
  - 적응성에 따라 ‘잘못된 답’으로 이끄는 뉴런들 사이의 연결은 약화되고, ‘올바른 답’으로 이끄는 연결은 강화됨
- 인공 신경망을 이용한 뇌 모델링
  - 인간 뇌를 기반으로 모델링 함.
  - 인간 뇌의 적응성을 활용하여 ‘학습 능력’을 구현함.
  - 인공 신경망은 아직 인간의 뇌를 흉내내기에 미흡하다.
- 인공 신경망의 학습 -> 인공 신경망의 가중치 조정

<table style="width:85%" border="1">
 <thead>
  <tr>
   <th style="width:26%">구분</th>
   <th style="width:36%">기존의 컴퓨터</th>
   <th style="width:36%">인공신경망</th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td>처리소자의 개수</td>
   <td>10<sup>8</sup>개의 트랜지스터</td>
   <td>10<sup>10</sup>개의 뉴런</td>
  </tr>
  <tr>
   <td>처리소자의 속도</td>
   <td>10<sup>12</sup>Hz</td>
   <td>10<sup>2</sup>Hz</td>
  </tr>
  <tr>
   <td>학습기능</td>
   <td>없음</td>
   <td>있음</td>
  </tr>
  <tr>
   <td>계산 스타일</td>
   <td>중앙 집중식, 순차적인 처리</td>
   <td>분산 병렬 처리</td>
  </tr>
 </tbody>
</table>

<br>

### 3. 초기의 신경망

**(1) 맥컬럭-피츠 뉴런과 헵의 학습 규칙**

- 각 신경세포 (neuron) 의 기능은 매우 단순하나, 이들이 상호 연결됨으로써 복잡한 계산을 수행하는 신경 시스템의 기초 마련
- 두 뉴런 사이의 연결강도를 조정할 수 있는 ‘헵의 학습 규칙’
- 뉴런 i와 뉴런 j 사이에 연결강도 wij가 존재

**(2) 로젠블럿의 단층 퍼셉트론**

- 최초의 신경망 모델인 ‘퍼셉트론’ (단층으로 이루어짐)
  - 신경세포와 유사한 단순 계산기능을 갖는 요소로 구성된 입력층과 출력층을 갖는 신경 시스템의 모델 제시
  - 입력과 출력 사이의 synapse 를 출력층의 제곱오차가 최소가 되는 방법으로 학습시킬 수 있음을 보여 줌

**[마크Ⅰ퍼셉트론]**

- 1957년에 제작된 신경망 하드웨어 장치
- A, B, C 등의 문자를 인식
- 당시 큰 센세이션을 불러일으킴
- 미국 전역에서 5,000번이 넘는 순회전시회가 열림
- 머지않아 가상의 사이버네틱스의 세계가 열릴 것을 기대
- 20 × 20개의 화소(pixel)를 가진 마크 I 퍼셉트론 화면
- 마크 I 퍼셉트론에서 연결선으로 연결강도 조정
- 학습을 위해 몇 km나 되는 연결선을 사용

**(3) 단층 퍼셉트론의 구조 : 뉴런의 입출력 구조와 비선형 함수**

- 뉴런에 해당하는 노드는 비선형적(non-linear)
- n개의 입력과 n개의 연결강도 벡터가 각각 곱해진 결과의 합이 활성 함수(activation function)에 의해 판단됨
- 그 값이 임계값(보통 0)보다 크면 1, 아니면 –1을 출력
- 노드의 출력값은 임계값(threshold), 오프셋(offset) θ, 비선형 함수의 형태에 따라 정해짐

**신경망에서 사용되는 대표적인 비선형 활성 함수**

- 계단함수
- 임계논리 함수
- S자 형태의 시그모이드(sigmoid) 함수(주로 퍼셉트론) 등

**(4) 퍼셉트론의 학습 과정**<br>
연결 강도를 조정하며 학습<br>
[1] 연결강도들과 임계값을 초기화<br>
[2] 새로운 입력과 기대되는 출력을 제시<br>
[3] 실제 출력값을 계산<br>
[4] 연결강도를 재조정<br>
[5] 더 이상 조정이 없을 때까지 [2] 단계로 가서 반복 수행

<img src="/assets/images/2025/11/25/251125-1.png"/><br>

**(5) 선형 분리 가능(Linear separability)**

- 한 직선에 의해 두 개 영역으로 분리되는 것
- AND와 OR 함수는 좌표가 한 직선에 의해 분리 가능
- AND에서 (1, 1)일 때만 좌표상 출력이 1(빨간점)
- 빨간색 점과 파란색 점의 클러스터로 분류

**XOR 함수와 선형 분리 불가능**

- Exclusive-Or(XOR) 함수는 선형 분리가 불가능
- 하나의 직선이 아닌 곡선에 의해서만 분리가 가능

**단층 퍼셉트론의 한계점**

- 단층 퍼셉트론 학습의 한계
- 단층 퍼셉트론은 입력 행렬과 결정 노드 사이에 단 하나의 가변적인 연결강도만을 가진 장치. 따라서 XOR 함수를 수행해내지 못하는 문제점 내포
- 단층 퍼셉트론은 선형 분리 가능한 함수만 학습할 수 있다.
- 선행 분리 가능한 함수들이 많지 않다는 점을 고려해 볼 때 좋은 모형은 아니다.
- 그러나 1980년대 중반 다층 퍼셉트론 모델의 기반이 됨
- 문자인식을 비롯한 여러 분야에 폭넓게 응용되었음
- 신경망 연구의 새로운 장을 열게 된 결정적인 계기<br><br>

### 4. 다층 퍼셉트론

**(1) 새로운 신경망 시대의 도래**

- 1980년대 중반에 다층 퍼셉트론 모델이 제안됨
- 단층 퍼셉트론 모델에다 하나 이상의 은닉층을 추가로 사용

**PDP(병렬분산처리 (Parallel Distributed Processing)) 그룹의 활약**

- 1986년 PDP 그룹의 러멜하트 등
- 『Parallel Distributed Processing(PDP)』 출간
- 신경망의 새로운 붐을 일으키는데 크게 기여
- 역전파 알고리즘을 널리 유행시킴

**(2) 다층 퍼셉트론의 구조와 학습 알고리즘**

- 입력층과 출력층 사이에 하나 이상의 은닉층 사용
- 입력층, 은닉층, 출력층의 순서와 방향으로 연결
- 각 층 내의 연결은 없음
- 출력층에서 입력층으로의 직접적인 연결도 없음

**다층 신경망 구조**

- **입력층**
  - 외부에서 받아들인 입력 신호를 은닉층의 모든 뉴런으로 보냄
  - 계산을 위한 뉴런은 거의 들어 있지 않음.
- **출력층**
  - 은닉층에서 출력 신호, 즉 자극 패턴을 받아 들이고 전체 신경망의
    출력 패턴 결정
- **은닉층**
  - 입력의 특성 파악
  - 뉴런의 가중치는 입력 패턴에 숨겨져 있는 특성을 나타냄
  - 출력층이 출력 패턴을 정할 때, 이 특성을 사용함
  - 은닉층은 목표 출력을 ‘숨기고’. 은닉층의 목표 출력은 해당 층에서 자체적으로 결정됨.
  - 신경망에 은닉층이 두 개 이상 들어갈 수 있다.

**다층신경망의 학습**

- 출력 패턴을 계산하고 실제와 목표 출력 간에 차이가 있다면 이 오차를 줄이도록 가중치 조절
- 역전파 학습
  - 입력이 주어지면 순방향으로 계산하여 출력을 계산한 후에 실제 출력과 우리가 원하는 출력 간의 오차를 계산한다.
  - 이 오차를 역방향으로 전파하면서 오차를 줄이는 방향으로 가중치를 변경한다

**역전파 알고리즘**

- 학습 알고리즘의 두 단계<br>
  ① 훈련 입력 패턴을 신경망의 입력층에 전달<br>
  ② 신경망은 출력층에서 출력 패턴이 생성될 때까지 층에서 층으로 입력 패턴을 전파
- 출력 패턴이 목표 패턴과 다르면 그 오차를 계산한 후 출력층에서 입력층까지 신경망을 따라 거꾸로 전파
- 오차가 전파되면서 가중치가 수정됨

- **역전파 학습 법칙 : 역전파 망 구조**
  Ex) 층이 세 개인 신경망
  - 인덱스 i, j, k는 각각 입력층, 은닉층, 출력층 뉴런.

<img src="/assets/images/2025/11/25/251125-2.png"/><br>

**가중치 업데이트 요약**

- 초기 가중치는 랜덤
- 예측 → 오차 발생
- 역전파가 어떤 가중치가 오차에 얼마나 기여했는지 계산
- 가중치를 조금 수정
- 오차 감소
- 수천 번 반복하면 정확도가 올라감

**신경망 구축 절차**

**1단계 : 데이터 수집**<br>
&nbsp;&nbsp;&nbsp;&nbsp;이미지, 텍스트, 표 데이터 등<br>
**2단계 : 전처리**<br>
&nbsp;&nbsp;&nbsp;&nbsp;정규화, 원-핫 인코딩, 데이터 분리(train/valid/test)<br>
**3단계 : 모델 설계**<br>
&nbsp;&nbsp;&nbsp;&nbsp;층 수, 뉴런 수, 활성화 함수 설정<br>
**4단계 : 학습**<br>
&nbsp;&nbsp;&nbsp;&nbsp;순전파 + 역전파 반복<br>
**5단계 : 검증**<br>
&nbsp;&nbsp;&nbsp;&nbsp;보지 않은 데이터에서 성능 측정<br>
**6단계 : 배포**<br>
&nbsp;&nbsp;&nbsp;&nbsp;새로운 데이터 입력 → 예측

**XOR 함수를 구현할 수 있는 신경망**

- XOR 함수를 간단한 다층 신경망으로 해결
- 선형 분리 문제에 대한 우려 완전 해소<br><br>

**Scikit-learn vs TensorFlow**

- 머신러닝 분야에서 필수적인 라이브러리이지만, 신경망(Neural Network)을 다루는 깊이와 목적에 큰 차이가 있음
- **Scikit-learn**은 빠르고 간단한 신경망 구현(교육용, 벤치마크용)에 적합
  - 신경망도 지원하는" 머신러닝 라이브러리
- **TensorFlow**는 복잡하고 거대한 딥러닝 모델 개발(실무용, 연구용)
  - 신경망을 위해 태어난" 딥러닝 프레임워크

<img src="/assets/images/2025/11/25/251125-3.png"/><br>

<table style="width:85%" border="1">
<caption>다층 퍼셉트론 모델의 기타 응용</caption>
 <thead>
  <tr>
   <th style="width:30%">분야</th>
   <th style="width:70%">예시</th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td>컴퓨터 비전</td>
   <td>이미지 분류, 객체 검출</td>
  </tr>
  <tr>
   <td>자연어 처리</td>
   <td>ChatGPT, 번역, 감정 분석</td>
  </tr>
  <tr>
   <td>음성</td>
   <td>음성 인식</td>
  </tr>
  <tr>
   <td>헬스케어</td>
   <td>영상 진단</td>
  </tr>
  <tr>
   <td>비즈니스</td>
   <td>추천 시스템, 사기 탐지</td>
  </tr>
  <tr>
   <td>시계열 분석</td>
   <td>주가 예측, 센서 분석</td>
  </tr>
 </tbody>
</table>

<table style="width:85%" border="1">
<caption>자주 발생하는 문제와 해결책</caption>
 <thead>
  <tr>
   <th style="width:30%">문제</th>
   <th style="width:70%">해결 방법</th>
  </tr>
 </thead>
 <tbody>
  <tr>
   <td>과적합(overfitting)</td>
   <td>정규화, dropout, 데이터 증가</td>
  </tr>
  <tr>
   <td>기울기 소실</td>
   <td>ReLU 사용, Batch Normalization</td>
  </tr>
  <tr>
   <td>학습 속도 느림</td>
   <td>GPU 사용, mini-batch, Adam</td>
  </tr>
  <tr>
   <td>성능 저하</td>
   <td>아키텍처 변경, 학습률 튜닝</td>
  </tr>
 </tbody>
</table>

<br>

### 5. 텐서플로우

- **텐서플로우(TensorFlow)**는 딥러닝 프레임워크의 일종
- 텐서플로우는 내부적으로 C/C++로 구현되어 있고 파이썬을 비롯하여 여러 가지 언어에서 접근할 수 있도록 인터페이스 제공

- **Keras**는 Python으로 작성되었으며 TensorFlow , CNTK 또는 Theano에서 실행할 수 있는 고수준 딥러닝 API이다.
- 쉽고 빠른 프로토타이핑이 가능하다.
- 순방향 신경망, 컨볼루션 신경망과 반복적인 신경망은 물론 여러 가지의 조합도 지원한다.
- CPU 및 GPU에서 원활하게 실행된다.
- 하위 클래스
  - models
  - layers
  - optimizers
